{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e33571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Face could not be detected in numpy array.Please confirm that the picture is a face photo or consider to set enforce_detection param to False.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     ret,frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m---> 14\u001b[0m     result \u001b[38;5;241m=\u001b[39m DeepFace\u001b[38;5;241m.\u001b[39manalyze(frame, actions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     15\u001b[0m     prediction_dict_1 \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     17\u001b[0m     gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\deepface\\DeepFace.py:233\u001b[0m, in \u001b[0;36manalyze\u001b[1;34m(img_path, actions, enforce_detection, detector_backend, align, expand_percentage, silent)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze\u001b[39m(\n\u001b[0;32m    150\u001b[0m     img_path: Union[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray],\n\u001b[0;32m    151\u001b[0m     actions: Union[\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m     silent: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    157\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    158\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m    Analyze facial attributes such as age, gender, emotion, and race in the provided image.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;124;03m            - 'white': Confidence score for White ethnicity.\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m demography\u001b[38;5;241m.\u001b[39manalyze(\n\u001b[0;32m    234\u001b[0m         img_path\u001b[38;5;241m=\u001b[39mimg_path,\n\u001b[0;32m    235\u001b[0m         actions\u001b[38;5;241m=\u001b[39mactions,\n\u001b[0;32m    236\u001b[0m         enforce_detection\u001b[38;5;241m=\u001b[39menforce_detection,\n\u001b[0;32m    237\u001b[0m         detector_backend\u001b[38;5;241m=\u001b[39mdetector_backend,\n\u001b[0;32m    238\u001b[0m         align\u001b[38;5;241m=\u001b[39malign,\n\u001b[0;32m    239\u001b[0m         expand_percentage\u001b[38;5;241m=\u001b[39mexpand_percentage,\n\u001b[0;32m    240\u001b[0m         silent\u001b[38;5;241m=\u001b[39msilent,\n\u001b[0;32m    241\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\deepface\\modules\\demography.py:120\u001b[0m, in \u001b[0;36manalyze\u001b[1;34m(img_path, actions, enforce_detection, detector_backend, align, expand_percentage, silent)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# ---------------------------------\u001b[39;00m\n\u001b[0;32m    118\u001b[0m resp_objects \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 120\u001b[0m img_objs \u001b[38;5;241m=\u001b[39m detection\u001b[38;5;241m.\u001b[39mextract_faces(\n\u001b[0;32m    121\u001b[0m     img_path\u001b[38;5;241m=\u001b[39mimg_path,\n\u001b[0;32m    122\u001b[0m     detector_backend\u001b[38;5;241m=\u001b[39mdetector_backend,\n\u001b[0;32m    123\u001b[0m     grayscale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    124\u001b[0m     enforce_detection\u001b[38;5;241m=\u001b[39menforce_detection,\n\u001b[0;32m    125\u001b[0m     align\u001b[38;5;241m=\u001b[39malign,\n\u001b[0;32m    126\u001b[0m     expand_percentage\u001b[38;5;241m=\u001b[39mexpand_percentage,\n\u001b[0;32m    127\u001b[0m )\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_obj \u001b[38;5;129;01min\u001b[39;00m img_objs:\n\u001b[0;32m    130\u001b[0m     img_content \u001b[38;5;241m=\u001b[39m img_obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mface\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\deepface\\modules\\detection.py:86\u001b[0m, in \u001b[0;36mextract_faces\u001b[1;34m(img_path, detector_backend, enforce_detection, align, expand_percentage, grayscale)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(face_objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m enforce_detection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m img_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     87\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFace could not be detected in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease confirm that the picture is a face photo \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor consider to set enforce_detection param to False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m         )\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     93\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFace could not be detected. Please confirm that the picture is a face photo \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor consider to set enforce_detection param to False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Face could not be detected in numpy array.Please confirm that the picture is a face photo or consider to set enforce_detection param to False."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "cap = cv2.VideoCapture(1)\n",
    "#check if the webcam is opened correctly\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "    \n",
    "while True:\n",
    "    ret,frame = cap.read()\n",
    "    result = DeepFace.analyze(frame, actions = ['emotion'])\n",
    "    prediction_dict_1 = result[0]\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #print(faceCascade.empty())\n",
    "    \n",
    "    faces = faceCascade.detectMultiScale(gray,2.5,5)\n",
    "    #Draw a rectangle around the faces\n",
    "    for(x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "    \n",
    "    font_c = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    #Use puttext() method for inserting text on video\n",
    "    cv2.putText(frame,prediction_dict_1['dominant_emotion'],(50,50),font_c,2,(0,0,255),2,cv2.LINE_4)\n",
    "    \n",
    "    cv2.imshow('Real Time Human Expression Detector', frame)\n",
    "    \n",
    "    if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7035ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
